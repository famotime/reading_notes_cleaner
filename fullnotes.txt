ChatGPT的工作过程更接近我们的大脑：在形成智能能力之前，会有大量原始、未经处理的数据通过输入端进入ChatGPT的数据库，这些数据库中的杂乱原始数据会在ChatGPT中进行融合，利用上下文，按照某种意义的组合方式进行结构化处理，形成类似于我们大脑中“信息”的数据处理结果，这些数据处理结果与处理前的原始数据，在ChatGPT的后端通过经验或按照特定主题进行理解和分析处理，形成了ChatGPT的“知识”，利用ChatGPT背后的人工智能技术和量子计算机进行进一步处理，使ChatGPT拥有了“智慧”。经过从“数据—信息—知识—智慧”的处理过程，ChatGPT拥有了像人类一样根据知识、经验和洞察力的结合从而作出正确判断和决策的能力。
从2018年起，OpenAI开始发布生成式预训练语言模型GPT（Generative Pre-trained Transformer），可用于生成文章、代码、机器翻译、问答等各类内容。2019年2月发布的GPT-2参数量为15亿，同年3月OpenAI宣布从“非营利”性质过渡到“封顶营利”性质，利润上限为任何投资的100倍。2019年7月22日，微软投资OpenAI 10亿美元，并获得了OpenAI技术的商业化授权，宣布双方将携手合作替Azure云端平台服务开发人工智能技术，从此OpenAI的一些技术开始出现在微软的产品和业务中。2020年6月11日，OpenAI宣布了GPT-3语言模型，参数量达到了1750亿。与此同时，OpenAI发布了OpenAI API，这是OpenAI第一个商业化产品，OpenAI正式开始了商业化运作。2020年9月22日，微软获得使用GPT-3模型的独家授权，使之成为全球首个享用GPT-3能力的公司。2023年3月14日，OpenAI正式发布了升级后的GPT-4。与之前相比，GPT-4不仅展现了更加强大的语言理解能力，还能够处理图像内容，在考试中的得分甚至能超越90%的人类。
Claude在工作原理上和ChatGPT十分相似，分为监督学习和强化学习两个阶段，Anthropic将这项技术称为原发人工智能，主要是靠强化学习来训练偏好模型，并进行后续微调。在监督学习阶段，Claude首先会对初始模型进行取样，进而根据模型结果继续产生自我修订，并根据修订效果对模型进行微调。在强化学习阶段，Claude会在监督学习结果的基础上继续对微调模型进行取样，基于Anthropic打造人工智能偏好数据集训练的偏好模型，作为奖励信号进行强化学习训练。Claude采用的原发人工智能方法与ChatGPT采用的人类反馈强化学习（Reinforcement Learning with Human Feedback, RLHF）最大的区别在于，Claude是基于偏好模型而非人工反馈来进行训练的，这种方法又被称为人工智能反馈强化学习（Reinforcement Learning from AI Feedback, RLAIF）。
词向量就是把一个单词转换成对应向量的表达形式，可以让计算机读取。在我们对单词做词向量编码的时候，不能只简单考虑当前的这个单词，而是要全局考虑当前这个词的上下文语境。要将整个上下文语境融入这一个单词的词向量当中。因为同样一个单词在不同的上下文中，表示的意思可能是不一样的。例如，“这次没考好没关系，不要有思想包袱”与“小明背着他的小包袱，开开心心地和同学们去郊游”两句话中同样是“包袱”，但所表达出来的意思完全不同，这种现象在自然语言处理中是常见的。
在Transformer中，采用了自注意力机制，将上下文信息融入每个单词，以增强单词的表示，解决了深度网络记性差的问题。它的原理就是计算文本中的每一个词与所有词之间的“关系”。例如，有这么一句话，“今天空气很新鲜”，它们对应的词向量分别是V（今天）、V（空气）、V（很）、V（新鲜）。通过self-attention计算每个单词与所有单词之间的关系，可以得到“新鲜”这个单词的词向量V（新鲜）=0.3*V（今天）+0.3*V（空气）+0.2*V（很）+0.2*V（新鲜）。如果对每个词向量都做这样的操作，那么每个单词除记录了单词本身，还记录了上下文的信息。同时，Transformer还采用了多头注意力机制来捕获更多、更丰富的特征。Transformer就是这样一个基于注意力机制的网络结构，它可以将句子中每个单词与句子中所有的单词（包括要计算的这个单词自己）进行并行计算，得到这个词与句子中每个单词的“关系”，从而确定这个词在整个句子里准确的意义，成功突破了时序序列的屏障。
2018年，一篇名为Improving Language Understanding by Generative Pre-Training的论文开启了GPT的新篇章，GPT-1正式与世人相见了。GPT-1的模型结构比较简单，包括12层Transformer，每一层拥有12个注意力头。它使用的BooksCorpus数据集，大概拥有5GB的文本量，1.17亿参数量。从这些数据可以看出，GPT-1并不是很大。GPT-1是自回归语言模型的代表。所谓自回归语言模型，我们可以将其简单理解为就是一个词一个词地往后进行预测，主要是通过在无标签的数据上学习到一个通用的语言模型，之后再根据特定的任务进行微调。对于GPT-1来说，所有的下游任务都需要再做微调。
2019年，可能大家熟知GPT-2是从一句“Too Dangerous To Release”的回复开始的。OpenAI称GPT-2如果不加任何限制就直接开源发布的话，实在是太危险了。GPT-2无论是文本量还是参数量都比GPT-1大了很多。GPT-2通过抓取美国社交新闻网站Reddit上拥有点赞数超过3个的文章，获得了大概40GB的文本数据，称其为WebText。同时模型结构也较GPT-1有轻微改变，模型规模也大了很多，参数量增加到了15亿。当然，主体上还是Transforme模型。与GPT-1的语言理解工作不同，GPT-2发现了它更擅长的东西——生成。
。如果是1-gram语言模型，则P（小猫、在、院子、里面、晒、太阳）=P（小猫）P（在）P（院子）P（里面）P（晒）P（太阳）。如果是2-gram语言模型，则P（小猫、在、院子、里面、晒、太阳）=P（小猫）P（在|小猫）P（院子|在）P（里面|院子）P（晒|里面）P（太阳|晒）。N-gram语言模型以此类推。当然基于这样统计假设的语言模型一定会有它的弊端，它对前序任务如分词、词性标注、实体抽取步骤的依赖性很强，如果前序工作没有做好的话，它的工作效果也会受到影响。
这个神经网络模型有预测的功能，当向其输入一句话中的前面几个单词的时候，神经网络就会输出紧接着的单词应该会是什么。与上面讲的概率统计语言模型一样，它也是通过概率来判断的。只不过这里利用的不是统计学，而是用神经网络来计算句子的概率，并提出了可以用数学的方式去表示每个词语。例如，大家较为熟悉的词袋模型（Bag-of-words model），它是一种在自然语言处理和信息检索下被简化的表达模型。我们可以形象地解释，意为将文本表示为装着其中单词的袋子。但是这种表示方式只保存了每个词出现的次数，而没有表示它们出现的顺序。
沉寂了10年，直到2013年才真正引起了人们的关注。因为在这一年谷歌推出了一个影响自然语言处理领域技术的里程碑式的模型——词向量模型，它将输入的词编码转化成为稠密的向量，并且如果是相似的词，它们对应的词向量也会相近。它在本质上也是神经概率语言模型，遵循着这样的基本思想：在一个句子中，一个单词只与在它周围的若干单词相关性较强，而与其他那些不在它周围的单词相关性较差。根据这样的基本思想，可以构建神经网络对当前单词和其上下文单词进行模型训练，最终得到词向量。这样从一定程度上可以表示出当前单词与上下文之间的关系。
k均值聚类算法的前提条件就是任意一组包含的数据数量完全相同。如果不满足这个前提条件，那么k均值聚类算法得出的结果将会完全错误甚至完全不着边际。这里的k就是指定分类的组数，具体需要分多少组，由k来决定，k由人类事先指定。k均值聚类算法主要应用在文档分类器、识别犯罪地点、物品传输优化、客户分类、球队状态分析、保险欺诈检测、乘车数据分析、网络分析犯罪分子等方面。如果要说起k均值聚类算法在分类问题上的应用，就不得不提文档分类器。文档数据标签、主题和文档内容将文档分为多个不同的类别。第一步操作就是使文档恢复默认状态，用一组数来表示每一个文档，接下来就是最重要的一步，统计术语出现的次数，次数越多越常用，标识出来并把内容类似的术语归到一类，从而就可以辨别出不同文档是否相近了。通过识别文档组中的相似性从而实现文档的分类。
人工神经元和真正的人脑神经元工作方法是一致的：从多个感受器接受电信号（0或者1表示），进行处理（加权相加），处理结果要和阈值相比较，根据是否大于阈值作出相应的判断，从而发出相应的电信号（正确就发出1，否则就发出0），这就是它叫神经元的原因。这里所说的阈值就是传递给各个信息元的数值。输入的信号乘以一定的权重，这里的权重是实数，可以为正，也可以为负，可以是整数，也可以是小数，具体数值可以根据需求随意设定，然后将乘积全部相加，得到的结果和阈值进行比较，如果大于等于阈值，就输出1，如果小于阈值就输出0。0和1对应了神经元的两种状态：抑制与兴奋。很多人工神经元组合在一起，设定好权重，计算机就可以实现各种不同信息的处理。这就是所谓的神经网络。
随之而来的便是人工神经网络的第一次低谷。因为美国科学家、“人工智能之父”马文·明斯基（Marvin Minsky）在1969年发现感知机存在致命缺点，就是它无法解决线性不可分问题。直观地说，就是感知机无法解决一条线不可分割的问题，如果x轴和y轴分别对应了不同年龄的人的身高、体重。按12岁以上和12岁以下进行分类，基本上一条直线就可以将所有的数据分开，但是如果按收入分类就无法用一条线分开，因为身高、体重和收入没有必然联系。这就是线性不可分问题。
在这个过程中，输入、输出是人类可以看到的，但是中间层是隐藏起来看不到的。正是在中间层（隐层）的位置，计算机完成了自动识别特征的工作。例如，一幅像素为1024的图像，输入层和输出层都是1024个像素点，但是隐层有300个特征点。这种1024到300的压缩，就是统计学中常用的“主成分分析法”。到了第三层，数据点又被压缩到100个，这样逐层压缩，但是越压缩，特征越抽象也越准确。生成的抽象度和准确度都高的特征，使深度学习在输出时就可以正确地输出，还原输入的数据。
2021年8月，美国华裔科学家、谷歌云的首席科学家李飞飞院士联合100多位学者发表一份题为On the Opportunities and Risk of Foundation Models的研究报告，这份报告有200多页，该报告详细描述了当前大规模预训练模型面临的机遇和挑战。在报告中，大模型被统一命名为Foundation Models，中文翻译为基础模型或者基石模型。该报告肯定了Foundation Models对智能体基本认知能力的推动作用。
它可以为用户打造一个独有的去中心化的通用数字身份体系，能在所有平台上使用，避免了创建账户设置密码的诸多问题。这个通用的数字身份体系是通过公私钥的签名与验签机制建立的，同时还通过分布式账本技术构建分布式公钥基础设施（Distributed Public Key Infrastructure, DPKI）和一种全新的可信分布式数字身份管理系统。用户数据经密码算法保护后在分布式账本上存储。身份信息与谁共享、作何种用途均由用户决定，只有经用户签名授权的个人数据才能被合法使用。通过数据的全生命周期确权，数据主体的知情同意权、访问权、拒绝权、可携权、删除权（被遗忘权）、更正权、持续控制权能得到真正的保障。而且分布式账本是一种严防篡改的可信计算范式，在此范式下，发证方、持证方和验证方之间可以端到端地传递信任，用户完全不用担心安全问题。Web3.0打破了中心化模式下数据控制者对数据的天然垄断，给予用户自主管理身份的权利。
另一方面，由于Web3.0各平台之间是基于统一的分布式协议进行连接，用户如果想更换平台也是极其简单的，只要花费极小的成本甚至无成本地就可以从一个提供商转移到另一个提供商。这是一种自由的双向选择关系，用户与平台完全处于平等地位，而不像Web2.0时代，由于平台之间的信息无法互通，也许最初用户还拥有选择的权利，但一旦开始注册使用，就被平台牢牢绑定，如果想更换平台，虽然也可以，但是意味着你要抛弃在当前平台上的所有数据，尤其是那些消失在历史洪流里的平台，用户数据更是不可能再找回。在Web3.0模式下，用户和平台是对等的关系，不存在谁控制谁的问题，这也是它的显著优势之一。
大家目前广泛使用的各种转账功能，看似是点对点发送价值，其实不然，在其背后，是有可信赖机构的账户系统为大家背书，进行价值的登记、流转、清算与结算。而Web3.0可以保证用户安全地进行金融交易，而且无须集中授权或协调者，这同样依赖于分布式账本技术。分布式账本技术为数字资产提供了独一无二的权益证明。利用哈希算法辅以时间戳生成的序列号是数字资产唯一性的保证，难以被复制。一人记录、多人监督复核的分布式共识算法杜绝了在没有可信中间人的情况下数字资产造假的问题。数字资产还能做到不可分割，可以完整状态存在、拥有和转移。
Web1.0和Web2.0以互联网平台为核心，由互联网平台组织开展信息生产与收集，通过平台连接产生网络效应，降低生产者与消费者之间的搜寻成本，优化供需匹配，因此被称为平台经济。而Web3.0利用分布式账本技术，构建一个激励相容的开放式环境，我们称之为去中心化自治组织（DAO）。在这样的环境中，众多互不相识的个体自愿参与“无组织”的分布式协同作业，像传统企业一样投资、运营、管理项目，并共同拥有权益和资产。项目决策依靠民主治理，由参与者共同投票决定，决策后的事项采用智能合约自动执行。去中心化自治组织是一种“无组织形态的组织力量”，没有董事会，没有公司章程，没有森严的上下级制度，没有中心化的管理者，而是去中心化，点对点平权。用户共创共建、共享共治，他们既是网络的参与者和建设者，也是网络的投资者、拥有者以及价值分享者。
2017年，谷歌的阿希什·瓦斯瓦尼（Ashish Vaswani）等提出Transformer架构，奠定了当前大模型领域主流的算法架构基础；Transformer结构的提出，使深度学习模型参数达到了上亿的规模。2018年，谷歌提出了大规模预训练语言模型Bert，该模型是基于Transformer的双向深层预训练模型，其参数首次超过3亿规模；同年，OpenAI提出了生成式预训练Transformer模型——GPT，大大地推动了自然语言处理技术领域的发展。此后，基于Bert的改进模型、ELNet、RoBERTa、T5等大量新式预训练语言模型不断涌现，预训练技术在自然语言处理技术领域蓬勃发展。2019年，OpenAI推出拥有15亿参数的GPT-2，其能够生成连贯的文本段落，做到初步的阅读理解、机器翻译等。紧接着，英伟达推出了拥有83亿参数的Megatron-LM，谷歌推出了拥有110亿参数的T5，微软推出了拥有170亿参数的Turing-NLG。2020年，OpenAI推出了超大规模语言训练模型GPT-3，其参数达到了1750亿，在两年左右的时间实现了模型规模从亿级到上千亿级的突破，并能够实现写诗、聊天、生成代码等功能。
此后，微软和英伟达在2020年10月联手发布了拥有5300亿参数的自然语言生成模型MT-NLG。2021年1月，谷歌推出的Switch Transformer模型以高达1.6万亿的参数量成为史上首个万亿级语言模型；同年12月，谷歌还提出了1.2万亿参数的通用稀疏语言模型GLaM，其在7项小样本学习领域的性能超过GPT-3。可以看到，大型语言模型的参数数量保持着指数增长势头。这样高速的发展并没有结束，2022年，又有一些常规业态大模型涌现，如StabilityAI发布的文字到图像的创新模型Diffusion，以及OpenAI推出的ChatGPT, ChatGPT是由效果比GPT-3更强大的GPT-3.5系列模型提供支持，并且这些模型使用微软Azure AI超级计算基础设施上的文本和代码数据进行训练。
ChatGPT被公认为是继互联网、智能手机之后，带给人类的第三次革命性产品。互联网开辟了“空间革命”，使人类可以实时与全世界链接，不必奔赴现场，可以通过互联网进行沟通、教学、视频会议，使政治、社会和商业等领域发生连锁变化。智能手机的出现带来了“时间革命”，通过可拓展安装的各种App应用软件，可以实现最快交易、最速送达，为人类的生活、工作和消费带来巨大变化。ChatGPT的横空出世，有望形成“思维革命”，替代人类进行创作、创意、解答、咨询、翻译、客服等，改变人类思考和处理问题的方式方法，并由此重塑各行业生态，甚至重塑整个世界。