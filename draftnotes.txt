一本书读懂ChatGPT
石子言、姚芳
前言
2023-07-07 07:39
2000多年前，先秦思想家鬼谷子有段名言：“以天下之目视者，则无不见；以天下之耳听者，则无不闻；以天下之心思虑者，则无不知。”在他看来，如果能集合天下所有的“眼睛”、“耳朵”和“心灵”，人类将“无所不见，无所不闻，无所不知”。
第一章 ChatGPT的前世今生
2023-07-07 07:41
第一章 ChatGPT的前世今生
一、ChatGPT的横空出世
2023-07-07 07:39
ChatGPT的全称是Chat Generative Pre-trained Transformer，是一种革命性的人工智能语言模型。
2023-07-07 07:41
（一）ChatGPT的开发团队——OpenAI
2023-07-07 07:42
OpenAI成立于2015年，由特斯拉首席执行官埃隆·马斯克（Elon Musk）、美国创业孵化器Y Combinator总裁山姆·阿尔特曼（Sam Altman）、全球在线支付平台PayPal联合创始人彼得·蒂尔（Peter Thiel）等硅谷科技大亨共同创办。
2023-07-07 07:42
OpenAI就将自己确立为一个使命驱动型的企业，其核心宗旨在于“实现安全的通用人工智能（AGI）”。
2023-07-07 07:42
2023年2月2日，OpenAI宣布推出ChatGPT Plus订阅服务，可以让用户在高峰期优先使用人工智能聊天机器人ChatGPT。
2023-07-07 07:42
据统计，2023年1月，平均每天有超过1300万名独立访问者使用ChatGPT，是2022年12月的两倍多。推出仅仅2个月，ChatGPT月活跃用户就成功过亿。
2023-07-07 07:42
（二）ChatGPT的爆火出圈
2023-07-07 07:43
在2023年1月末就已突破1亿，之前最快破亿的是TikTok（抖音海外版），而其用户破亿也用时9个月，这一切让ChatGPT成为史上用户增长速度最快的消费级应用程序。
2023-07-07 07:44
它被《科学》（Science）杂志评选为2022年度十大科学突破之一。
2023-07-07 07:44
ChatGPT在短短2个月内，就拥有了超1亿的活跃用户，美国89%的大学生在使用这个软件写作业，而这个数字还在不断上升中，
2023-07-07 07:45
ChatGPT的工作过程更接近我们的大脑：在形成智能能力之前，会有大量原始、未经处理的数据通过输入端进入ChatGPT的数据库，这些数据库中的杂乱原始数据会在ChatGPT中进行融合，利用上下文，按照某种意义的组合方式进行结构化处理，形成类似于我们大脑中“信息”的数据处理结果，这些数据处理结果与处理前的原始数据，在ChatGPT的后端通过经验或按照特定主题进行理解和分析处理，形成了ChatGPT的“知识”，利用ChatGPT背后的人工智...
2023-07-07 07:45
（三）ChatGPT带来的争议
2023-07-07 07:46
ChatGPT之所以颠覆了以往所有的聊天机器人的应用程序，其核心原因就是其人工智能水平真正达到了某个行业的专家水平。
2023-07-07 07:47
在专家们看来，如果你本身是一位行业专家，具备对本行业知识的判断力，那么ChatGPT将是你的另一个得力助手；如果你是在寻求一个你不了解领域的答案，对ChatGPT的回答要谨慎对待。
2023-07-07 07:47
二、ChatGPT的发展历程
2023-07-07 07:47
（一）聊天机器人的产生
2023-07-07 07:48
2016年是聊天机器人崛起的元年。微软发布了一款针对18岁到24岁青少年名为Tay的在线聊天机器人，除了人工智能技术，Tay的认知内容主要由包括即兴喜剧演员在内的作家精心创作而成，
2023-07-07 07:48
由于Tay初始状态对世界一无所知，并不具备独立思考的能力，于是在短短不到24小时内便被“教导”成为一个极端的种族主义者。在这种情况下，微软不得不将其紧急关停。
2023-07-07 07:49
（二）ChatGPT的技术演进
2023-07-07 07:50
从2018年起，OpenAI开始发布生成式预训练语言模型GPT（Generative Pre-trained Transformer），可用于生成文章、代码、机器翻译、问答等各类内容。2019年2月发布的GPT-2参数量为15亿，同年3月OpenAI宣布从“非营利”性质过渡到“封顶营利”性质，利润上限为任何投资的100倍。2019年7月22日，微软投资OpenAI 10亿美元，并获得了OpenAI技术的商业化授权，宣布双方将携手合作替A...
2023-07-07 07:50
OpenAI推出的GPT模型是一种自然语言处理模型，使用Transformer（多层变换器）来预测下一个单词的概率分布，通过训练在大型文本语料库上学习到的语言模式来生成自然语言文本。
2023-07-07 07:50
表1-1　GPT家族主要模型相关数据对比
2023-07-07 07:50
（三）ChatGPT的未来发展
2023-07-07 08:04
一是在ChatGPT中引入搜索技术。
2023-07-07 08:04
二是将ChatGPT与知识图谱结合。
2023-07-07 07:51
知识图谱此前作为搜索引擎的重要技术，其知识的构建往往是抽取式的，包含一系列知识冲突检测、消解过程，知识的每个构建环境都能溯源，而ChatGPT常常出现事实谬误的问题，利用知识图谱的技术和方法，可以增强ChatGPT的知识推理能力，并使知识可溯源，增强知识的可解释性
2023-07-07 08:04
三是让ChatGPT与多模态技术结合。
2023-07-07 08:05
四是还应让ChatGPT与具身智能结合。
2023-07-07 08:05
具身智能还可以与环境交互，真正让模型在现实中学习，投入现实世界，这是人工智能与人类“目标对齐”的重要技术路径。
2023-07-07 08:05
五是还需在ChatGPT中引入负责任人工智能技术。ChatGPT在拒绝回答自己不懂或其他敏感话题等方面已经进行了大量训练。
2023-07-07 08:06
三、ChatGPT的竞争力分析
2023-07-07 08:06
（一）ChatGPT的重要竞争对手
2023-07-07 08:10
说到Claude，就不得不提到它的开发团队——Anthropic。因不满老东家成为微软附庸，11名OpenAI前员工怒而出走，成立了名为Anthropic的新公司。
2023-07-07 08:10
在谷歌投资之前，Anthropic筹集了超过7亿美元的资金。“硅谷立场”网站报道称，谷歌之所以选择Anthropic，是因为它的主要研究人员曾是OpenAI的成员。谷歌和Anthropic合作是追赶微软在快速增长的人工智能市场中的领先地位的策略之一。
2023-07-07 08:12
这家硅谷新星公司目前估值50亿美元，如今带着ChatGPT最强竞品——Claude聊天机器人杀回战场。2023年1月，Anthropic已经公布正在测试中的新型聊天机器人Claude，目的就是与ChatGPT争锋。
2023-07-07 08:13
Claude在工作原理上和ChatGPT十分相似，分为监督学习和强化学习两个阶段，Anthropic将这项技术称为原发人工智能，主要是靠强化学习来训练偏好模型，并进行后续微调。在监督学习阶段，Claude首先会对初始模型进行取样，进而根据模型结果继续产生自我修订，并根据修订效果对模型进行微调。在强化学习阶段，Claude会在监督学习结果的基础上继续对微调模型进行取样，基于Anthropic打造人工智能偏好数据集训练的偏好模型，作为奖励信...
2023-07-07 08:14
。2021年5月，谷歌推出了大语言模型LaMDA。2022年初，其官方论文介绍，LaMDA模型使用多达137B个参数训练，展示了接近人类水平的对话质量。2023年2月7日凌晨，谷歌首席执行官桑达尔·皮查伊（Sundar Pichai）宣布，推出一款名为Bard的实验性对话人工智能服务，提供类似ChatGPT、由LaMDA模型支持的对话式人工智能服务。
2023-07-07 08:15
（二）ChatGPT的核心优势
2023-07-07 08:15
ChatGPT在对话过程中会记忆先前使用者的对话讯息，用于后续对话中完成上下文理解，这就使ChatGPT和大家在生活中用到的各类“人工智障”式智能音箱不同，它可以回答某些假设性的问题。同时，ChatGPT可实现连续对话，用户使用ChatGPT的对话交互体验感可得到极大地提升。ChatGPT性能和用户体验感的大大提高，主要在于其引入了新技术——基于人类反馈的强化学习（RLHF）。
2023-07-07 08:16
ChatGPT突出的优点在于，它可以主动承认自身错误。若用户指出其错误，模型会听取意见并优化答案；可以质疑不正确的问题。
2023-07-07 08:16
（三）ChatGPT目前有何局限
2023-07-07 08:16
其最近数据更新的时间截止于2021年，对于2021年后至今的任何事件，ChatGPT无法给出准确的答案。
2023-07-07 08:16
此外，ChatGPT回答的准确度是不可信任的，它在很多领域可以“创造答案”，
2023-07-07 08:17
因为ChatGPT并非100%稳妥可靠的，所以用户使用ChatGPT时需要具有鉴别回答质量与准确性的专业能力。
第二章 ChatGPT：最前沿的自然语言处理技术
2023-07-07 08:18
第二章 ChatGPT：最前沿的自然语言处理技术
一、ChatGPT的整体技术流程
2023-07-07 08:18
一、ChatGPT的整体技术流程
2023-07-07 08:18
（一）自然语言处理
2023-07-07 08:19
（二）ChatGPT的核心技术
2023-07-07 08:19
ChatGPT的两个核心技术，Transformer和人类反馈强化学习，
2023-07-07 08:20
在很长一段时间之内，RNN系列都是我们处理序列化问题的主要技术。但是它存在着一些让我们困惑的问题。例如，RNN模型无法实现并行化处理，因为在它的模型结构中，后续神经元的输入将等待前序神经元的输出。也就是说，在一个句子中，每个单词都要等待它前序单词的隐藏状态输出以后才能开始处理，这是一个典型的串行结构，对现在大规模并行化的处理模式非常不友好。
2023-07-07 08:20
还有另一个更让人头疼的问题，就是我们常说的“可怕的梯度消失”问题。这个问题可以导致RNN记性变得很差，并且长期困扰着我们。具体说来，就是如果输入的语句非常长，网络可能会出现遗忘现象。
2023-07-07 08:20
长，网络可能会出现遗忘现象。例如，我们想让机器生成这么一段话：“丽丽背着书包去上学，这是一个晴空万里的早上，小鸟在歌唱，太阳公公在微笑，路上碰到小朋友对（）说：‘早上好，我们一起上学校。’”那么这里应该用“他”还是“她”呢？由于这句话太长，网络到了括号这里就不记得前面到底说的是女孩还是男孩，括号内应该填写“他”还是“她”了。
2023-07-07 08:21
2017年，谷歌在NISP（NeurlPS，神经信息处理系统大会）2017上发表了一篇名为Attention Is All You Need的论文。这篇文章提出了大名鼎鼎的Transformer模型，从此Transformer在自然语言处理领域中大热起来。后来取得成功的Bert和GPT都归功于这个模型
2023-07-07 08:21
注意力机制。当我们听到一段话的时候，这段话中虽然有很多词，但是它们对这句话的贡献并不一样。我们的注意力总会在那么一两个关键词上。
2023-07-07 08:22
在该模型中，它是通过权重系数来实现的，那些重要的信息将会有更高的权重，而那些不太重要的信息权重值则非常低。
2023-07-07 08:22
词向量就是把一个单词转换成对应向量的表达形式，可以让计算机读取。在我们对单词做词向量编码的时候，不能只简单考虑当前的这个单词，而是要全局考虑当前这个词的上下文语境。要将整个上下文语境融入这一个单词的词向量当中。因为同样一个单词在不同的上下文中，表示的意思可能是不一样的。例如，“这次没考好没关系，不要有思想包袱”与“小明背着他的小包袱，开开心心地和同学们去郊游”两句话中同样是“包袱”，但所表达出来的意思完全不同，这种现象在自然语言处理中是...
2023-07-07 08:23
在Transformer中，采用了自注意力机制，将上下文信息融入每个单词，以增强单词的表示，解决了深度网络记性差的问题。它的原理就是计算文本中的每一个词与所有词之间的“关系”。例如，有这么一句话，“今天空气很新鲜”，它们对应的词向量分别是V（今天）、V（空气）、V（很）、V（新鲜）。通过self-attention计算每个单词与所有单词之间的关系，可以得到“新鲜”这个单词的词向量V（新鲜）=0.3*V（今天）+0.3*V（空气）+0.2...
2023-07-07 08:24
DeepMind和OpenAI联合发布的一篇名为Deep Reinforcement Learning from Human Preferences的论文对基于人类反馈的强化学习方法进行了描述。一般来说，强化学习的整个过程不会受到人类的干预，而人类反馈强化学习则是通过人工标注作为反馈，从而提升了强化学习的表现效果。
2023-07-07 08:28
三）揭开ChatGPT的神秘面纱
2023-07-07 08:32
。根据InstructGPT的原理作为参考，我们同样将ChatGPT整个学习流程分为三个步骤，这里将分别介绍每个步骤所需要做的事情。第一步：监督调优（SFT）模型。这一步主要是收集数据，训练有监督的策略模型。在训练一开始，需要做的第一件事就是选用一个非常经典的、实用的预训练语言模型作为初始模型。在ChatGPT中，选用的是GPT-3.5模型。
2023-07-07 08:32
ChatGPT采用的方法是找来了大量的人工对GPT-3.5模型进行修正。人们随机抽取了一些问题，并准确地给出答案，进行人工标注。将人类的知识告诉模型，例如，通过问题：“糖是甜的还是咸的？”标注答案：“糖是甜的。”通过问题：“中国的首都是哪里？”标注答案：“中国的首都是北京。”
2023-07-07 08:33
有了这些人工标记的数据就可以通过它们来微调GPT-3.5模型了。这个被微调过的GPT-3.5模型就被称为SFT模型。
2023-07-07 08:33
第二步：训练奖励模型（RM）。通过收集比较数据，来训练奖励模型。这一步的主要的目的就是训练奖励模型，这个模型可以对监督调优模型的输出进行打分，这个分数反映了被选定的人类标注者的一个偏好，当然这个偏好在大多数情况下是符合人类的共同认知的。使用奖励模型对监督调优模型的输出结果进行打分，表示这个输出与人类认知的一个契合度（当然，这个“人类”也是有局限性的，它受限于被选定的人类标注者的群体倾向）。
2023-07-07 08:34
现在我们要做的事情就是要让这个模型拥有和人类一样的认知倾向。它是如何做的呢？首先，收集比较数据。我们向监督调优模型输入一些问题，它会为每个问题生成多个输出。例如，“你认为西游记中哪个人物最厉害”，该模型可能会输出这样几个结果：“孙悟空”“猪八戒”“土地公”“如来佛”。这时将会找到许多不同的人类标注者，按照他们自己的认知，对输出的结果进行打分，然后按照分数从高到低进行排序
2023-07-07 08:34
这里会找很多人，将会产生很多种不同的排序。虽然不可能所有人给出的排序结果一模一样，但是这个结果应该是符合大众的认知的。例如，多数人应该不会认为“土地公”是西游记中最厉害的人物。得到比较数据后，将这些数据去训练一个奖励模型。
2023-07-07 08:35
第三步：使用近端策略优化模型（PPO）微调监督调优模型。这一步的目的就是用强化学习近端策略优化方法来微调监督调优模型。首先使用监督调优模型（第一步中得到的模型）初始化近端策略优化模型，价值模型为训练奖励模型（第二步中得到的模型）。随机向近端策略优化模型进行提问并得到回答，由奖励模型对回答进行评估和打分。通过这个分数更新近端策略优化模型，使模型得到的分数越来越高。其中第二步和第三步会迭代进行，使训练奖励模型和近端策略优化模型越来越强大。
2023-07-07 08:35
二、ChatGPT背后的GPT-3.5语言模型
2023-07-07 08:35
（一）预训练语言模型
2023-07-07 08:35
2017年，Transformer模型诞生了，它标志着一个新时代的开始，从此预训练语言模型开启了新篇章。
2023-07-07 08:36
预训练模型就很好地利用了迁移学习这一特点，它先学习知识，此时没有任何具体的任务。把基础知识学扎实之后，需要它做具体任务的时候，再进行微调。
2023-07-07 08:37
预训练语言模型就是在大规模的语料库上进行预先训练，从而学习到人类语言的知识，它是后期做具体任务的基础模型。我们这里说的火爆全球的ChatGPT则是基于预训练语言模型GPT-3.5优化后的一个模型。
2023-07-07 08:37
（二）GPT发展史
2023-07-07 08:38
2018年，有两个非常厉害的语言模型横空出世了。一个是GPT，另一个就是Bert。它们都是在超大规模的数据中心中训练出来的，其核心都是我们在本章第一节中所介绍的Transformer网络。
2023-07-07 08:39
Bert是谷歌公司推出的，它的拿手好戏是做完形填空，例如，“我喜欢（）的四季如春，有机会我一定要去昆明旅游”。Bert可以利用上下文来确定括号内应填写的这个词大概率是“昆明”。而与Bert擅长做完形填空不一样，GPT擅长根据前文来预测下一个词，例如，根据“我希望明天天气”来预测后面的单词是“好”。
2023-07-07 08:40
2018年，一篇名为Improving Language Understanding by Generative Pre-Training的论文开启了GPT的新篇章，GPT-1正式与世人相见了。GPT-1的模型结构比较简单，包括12层Transformer，每一层拥有12个注意力头。它使用的BooksCorpus数据集，大概拥有5GB的文本量，1.17亿参数量。从这些数据可以看出，GPT-1并不是很大。GPT-1是自回归语言模型的代表。...
2023-07-07 08:40
2019年，可能大家熟知GPT-2是从一句“Too Dangerous To Release”的回复开始的。OpenAI称GPT-2如果不加任何限制就直接开源发布的话，实在是太危险了。GPT-2无论是文本量还是参数量都比GPT-1大了很多。GPT-2通过抓取美国社交新闻网站Reddit上拥有点赞数超过3个的文章，获得了大概40GB的文本数据，称其为WebText。同时模型结构也较GPT-1有轻微改变，模型规模也大了很多，参数量增加到了1...
2023-07-07 08:41
一般来说，模型预测下一个单词时，会给出概率最大的那个。为了让生成的词语有更大的多样性，GPT-2可以对采样策略中的参数进行调整，实现多样性的输出。
2023-07-07 08:41
与GPT-1相比，GPT-2的真正强大之处在于，它可以去做Zero-Shot任务，Zero-shot任务也叫零样本任务。它只通过一则任务说明，没有任何范例，直接生成任务要求的数据。
2023-07-07 08:41
GPT-1对于所有下游的任务都需要再做微调，但对GPT-2来说就不需要了。它可以通过一些提示，暗示模型需要完成什么任务。例如，我们想让模型做一项翻译任务，只需要给模型一段提示。例如，“请将中文翻译成英文：学生=＞”，模型会输出“student”。
2023-07-07 08:41
时间来到了2020年，GPT-3问世了。相较于前一个版本GPT-2，它的模型更大了，数据也更多了。除了之前GPT-2用的WebText数据集的加强版，还增加了图书、维基百科、Common Crawl等数据集，共45TB（太字节）的文本，其中纯文本就达到了570GB（吉字节）。并且其中的数据包罗万象，可以说它上通天文、下知地理。GPT-3坐拥1750亿个参数，是GPT-2的100多倍。
2023-07-07 08:42
GPT-3在GPT-2的基础上继续进行了改进。除了GPT-2中的Zero-shot任务，GPT-3增加了2种任务，具有“Few-shot”，“One-shot”和“Zero-shot”3种核心的下游任务方式。
2023-07-07 08:42
One-shot任务，可以输入一个例子和一则任务说明，例如，“请将中文翻译成英文：朋友=＞friend，学生=＞”，模型会输出“student”。Few-shot任务，可以输入多个例子和一则任务说明。例如，“请将中文翻译成英文：朋友=＞friend，计算机=＞computer，数据=＞data，学生=＞”，模型会输出“student”。
2023-07-07 08:42
GPT-3.5重点的训练过程就是人类反馈强化学习。
2023-07-07 08:42
三、深度学习技术演进带来ChatGPT
2023-07-07 08:43
（一）基于统计学的自然语言处理技术
2023-07-07 08:44
世界著名的语音识别和自然语言处理专家弗莱德里克·贾里尼克（Frederek Jelinek）提出了一个假设：“一个句子是否合理，取决于它出现在自然语言中的可能性。”这就是统计学语言模型的基础原理，并在此原理基础之上提出了基于统计的语音识别框架，将当时的语言识别率从70%提高到了90%。
2023-07-07 08:44
对于语言序列，语言模型的任务就是计算该序列的概率，如计算句子“小猫在院子里面晒太阳”的概率。语言模型的任务就是对每个句子在语言中出现的概率进行预测。那么一个好的语言模型要做到的事情就是：对于一个合理的句子，语言模型应该给它一个相对高的概率，对于一个不合理的句子，语言模型应该给它一个相对低的概率，最好是趋近于零。
2023-07-07 08:45
例如，我们这里有一个句子，“小猫在院子里面晒太阳”。我们可以将它看成一系列单词的序列，如（小猫、在、院子、里面、晒太阳），统计语言模型可以赋予这个序列一个概率
2023-07-07 08:45
我们只需要计算P（小猫）、P（在|小猫）、P（院子|小猫、在）、P（里面|小猫、在、院子）、P（晒|小猫、在、院子、里面）以及P（太阳|小猫、在、院子、里面、晒）的值，然后把这些值相乘，就可以得到这个句子最终的打分了。
2023-07-07 08:46
P（小猫）这个参数非常容易得到，即“小猫”这个词出现的次数相对整个语料库中所有词语总数的比例，从这里看，其实就是做了一个计数工作。
2023-07-07 08:46
P（在|小猫），与刚刚计算P（小猫）不一样，这是一个条件概率，表示在“小猫”这个词出现之后，“在”这个词出现的概率。这里根据贝叶斯定理的思想，P（在|小猫）=C（小猫、在）/C（小猫），这里的C（小猫）表示“小猫”这个词在语料库中出现的次数，C（小猫、在）表示“小猫”“在”两个词同时在语料库中出现的次数，P（在|小猫）就是当“小猫”这个词出现的时候，“小猫”“在”同时出现的概率。
2023-07-07 08:46
如果这句话在语料库中没有出现过呢？这时统计学语言模型会给一个非常低的得分，但是这句话也有可能是符合要求的。
2023-07-07 08:47
由于模型会存在因语料库中没有出现而对语句进行误判的情况，N-gram语言模型对上述方法稍作了改变。N-gram语言模型依据马尔可夫（Markov）假设，未来的事件只取决于有限的历史信息。N-gram语言模型认为第N个词出现只与它前面的N-1个词相关。通过这样的假设，N-gram语言模型大大简化了语言模型的计算。
2023-07-07 08:47
。如果是1-gram语言模型，则P（小猫、在、院子、里面、晒、太阳）=P（小猫）P（在）P（院子）P（里面）P（晒）P（太阳）。如果是2-gram语言模型，则P（小猫、在、院子、里面、晒、太阳）=P（小猫）P（在|小猫）P（院子|在）P（里面|院子）P（晒|里面）P（太阳|晒）。N-gram语言模型以此类推。当然基于这样统计假设的语言模型一定会有它的弊端，它对前序任务如分词、词性标注、实体抽取步骤的依赖性很强，如果前序工作没有做好的话，...
2023-07-07 08:48
（二）深度学习分水岭
2023-07-07 08:48
2003年，一个在深度学习领域极具影响力的人物，2018年图灵奖获得者、人工智能先驱、深度学习三巨头之一的约书亚·本吉奥（Yoshua Bengio）发表了一篇论文A Neural Probabilistic Language Model。首次提出了用神经网络来训练语言模型。
2023-07-07 08:48
这个神经网络模型有预测的功能，当向其输入一句话中的前面几个单词的时候，神经网络就会输出紧接着的单词应该会是什么。与上面讲的概率统计语言模型一样，它也是通过概率来判断的。只不过这里利用的不是统计学，而是用神经网络来计算句子的概率，并提出了可以用数学的方式去表示每个词语。例如，大家较为熟悉的词袋模型（Bag-of-words model），它是一种在自然语言处理和信息检索下被简化的表达模型。我们可以形象地解释，意为将文本表示为装着其中单词的...
2023-07-07 08:49
沉寂了10年，直到2013年才真正引起了人们的关注。因为在这一年谷歌推出了一个影响自然语言处理领域技术的里程碑式的模型——词向量模型，它将输入的词编码转化成为稠密的向量，并且如果是相似的词，它们对应的词向量也会相近。它在本质上也是神经概率语言模型，遵循着这样的基本思想：在一个句子中，一个单词只与在它周围的若干单词相关性较强，而与其他那些不在它周围的单词相关性较差。根据这样的基本思想，可以构建神经网络对当前单词和其上下文单词进行模型训练，...
2023-07-07 08:50
标准的深度神经网络的输入层和输出层神经元的个数是固定的，但是我们的句子长度是不固定的。例如，这里需要模型做一个机器翻译的工作，假设我们输入的语句是“我是一名学生”。包括标点符号，它的长度是7，如果我们想让模型再翻译另外一句话，“今天的天气非常好”，包括标点符号，它的长度是9，那么我们如何设计一个固定输入数的神经网络来完成这个任务呢？
2023-07-07 08:58
循环神经网络是一种非常著名且实用的深度神经网络，在自然语言处理的多个任务上都取得了非常好的成绩。从此，循环神经网络就成了自然语言处理最常用的方法之一，它拥有一个不断循环隐藏层的结构，这样的结构特征具体地表现为每一个隐藏层的输入不仅受当前层输入的影响，还受到来自上一个时间步输出的影响。也就是说这个网络是具有记忆功能的。
2023-07-07 08:59
但正是因为循环神经网络以及类循环神经网络模型引入了时间步这个操作，因此它们必须等待上一步的操作结果出来后才能进入下一步的计算。也就是它们共同的一个特点：串行处理机制，这就导致了它们的并行化计算很难实现。
2023-07-07 08:59
直到2017年谷歌的一篇论文Attention Is All You Need发表，这标志着大名鼎鼎的模型Transformer出世了。Transformer使用注意力机制来捕获输入和输出之间的关系，从而使整个架构更加并行化。这也将自然语言处理的发展推向了一个新的时期——预训练语言模型时期。
第三章 ChatGPT开启智能领域新篇章
2023-07-07 08:59
第三章 ChatGPT开启智能领域新篇章
一、人工智能技术的发展
2023-07-07 09:00
一、人工智能技术的发展
2023-07-07 09:00
（一）机器学习是什么
2023-07-07 09:00
机器学习通常分为三类：监督学习、无监督学习和强化学习。
2023-07-07 09:01
监督学习大致可以分为两类：分类问题，这类问题的输出值是定性离散的；回归问题，这类问题的输出值是定量并且通常是连续的。
2023-07-07 09:01
邮件过滤器的规则就是把邮件中出现的诸如“促销”“打折”“贷款”等特征性敏感性的词语收集起来，形成一个庞大的词汇数据库，每个词赋予一定的权重。当收到新邮件的时候，用邮件中的内容跟这个数据库中的词汇去进行比较，如果权重值的总乘积达到某个百分比，这些新邮件就会被标记为垃圾邮件，然后被垃圾邮件过滤器过滤到垃圾箱
2023-07-07 09:02
回归问题主要用来预测一个具体的数值，能够给出具体数值。典型应用场景有房屋成交价格的预测、未来天气情况的预测等。
2023-07-07 09:03
这种自动找寻函数关系的过程，就是我们所说的机器学习。移动函数的多个参数，包括权重（a1、a2……an）、截距e，从中找出与所求问题关系最密切的数据，再将所有的参数数据都用一个函数来表示，就能根据多个影响因素推算出确定的降水量。
2023-07-07 09:03
是否输出明确的数值就是区分分类问题和回归问题的关键。
2023-07-07 09:03
陷入“过度学习”的最主要原因是计算机用来训练的模型参数过多，所以参数的选择并不是越多越好，一定要限定参数。
2023-07-07 09:04
监督学习有输入有输出，目的就是找寻输入输出数据间的关系，从而根据输入来给出预测输出值，每组输入输出数据对应一个点，监督学习就是要把这些点尽可能连接起来形成一条直线，这条直线对应的函数就是要找寻的关系表示函数。
2023-07-07 09:04
无监督学习不需要画出函数的那条直线，不需要费尽心思找到输入数据和输出数据之间的关系。这种令计算机分析不知道正确答案的数据，让其自己发现其中的特征和规律的方法就是无监督学习。
2023-07-07 09:05
企业销售员工对顾客购买群体进行分类就会用到无监督学习。因为销售员工想要了解顾客的购买倾向并不容易，不可能一开始就知道这位顾客要买什么，这时候就要用到无监督学习，让计算机对顾客进行分类，分好类之后，就可以给不同的顾客推荐适合他们的产品。例如，在淘宝网购物，有一个栏目叫作“猜你喜欢的商品”，通过这一栏就可以了解到更符合我们需求的产品。
2023-07-07 09:05
无监督学习最具代表性的分类方法就是聚类。俗话说：物以类聚，人以群分。聚类就是采用这种原则：将所有数据中的相似数据总结在一起。
2023-07-07 09:06
鉴于分类方法的多样性，在实际进行聚类的时候，要加上一个前提设定条件。
2023-07-07 09:06
k均值聚类算法的前提条件就是任意一组包含的数据数量完全相同。如果不满足这个前提条件，那么k均值聚类算法得出的结果将会完全错误甚至完全不着边际。这里的k就是指定分类的组数，具体需要分多少组，由k来决定，k由人类事先指定。k均值聚类算法主要应用在文档分类器、识别犯罪地点、物品传输优化、客户分类、球队状态分析、保险欺诈检测、乘车数据分析、网络分析犯罪分子等方面。如果要说起k均值聚类算法在分类问题上的应用，就不得不提文档分类器。文档数据标签、主...
2023-07-07 09:06
强化学习这个概念最早被大众熟知是2017年，AlphaGo在围棋比赛中战胜了当时世界排名第一的柯洁。强化学习和监督学习、无监督学习方式的最主要区别在于：强化学习训练时，需要环境给予反馈以及对应具体的反馈值。它的目的不是完成分类，不是将邮件区分为垃圾邮件和正常邮件诸如此类的任务，而主要是为了指导训练对象每一步如何决策，采用什么样的行动可以完成特定的目的或者使收益最大化。
2023-07-07 09:08
强化学习主要应用在自动驾驶、游戏、推荐系统等方面。自动驾驶是人工智能应用较为成熟的领域。
2023-07-07 09:08
目前市场上的很多MOBA（多人在线战术竞技）游戏基本都包含强化学习人工智能，其中最广为人知的就是《王者荣耀》人工智能。由于该游戏本身就是虚拟的，因此在游戏环境下计算机可以和外界随便交互，任意试错，不产生任何真实世界的成本，同时游戏本身的奖励也相对容易设置，存在明显的奖励机制。
2023-07-07 09:08
（二）神经网络又是什么
2023-07-07 12:27
人工神经元和真正的人脑神经元工作方法是一致的：从多个感受器接受电信号（0或者1表示），进行处理（加权相加），处理结果要和阈值相比较，根据是否大于阈值作出相应的判断，从而发出相应的电信号（正确就发出1，否则就发出0），这就是它叫神经元的原因。这里所说的阈值就是传递给各个信息元的数值。输入的信号乘以一定的权重，这里的权重是实数，可以为正，也可以为负，可以是整数，也可以是小数，具体数值可以根据需求随意设定，然后将乘积全部相加，得到的结果和阈值...
2023-07-07 12:29
随之而来的便是人工神经网络的第一次低谷。因为美国科学家、“人工智能之父”马文·明斯基（Marvin Minsky）在1969年发现感知机存在致命缺点，就是它无法解决线性不可分问题。直观地说，就是感知机无法解决一条线不可分割的问题，如果x轴和y轴分别对应了不同年龄的人的身高、体重。按12岁以上和12岁以下进行分类，基本上一条直线就可以将所有的数据分开，但是如果按收入分类就无法用一条线分开，因为身高、体重和收入没有必然联系。这就是线性不可分...
2023-07-07 12:29
（三）深度学习有多厉害
2023-07-07 12:30
在深度学习出现之前，训练计算机识别出动物图片，要给计算机输入各种动物的特征，如老虎额头有个王字，猫的趾底有厚的肉垫用来确保行走无声，等等。这些特征都是人来提取描述的，这类工作还有个专有名词叫作“特征工程”。
2023-07-07 12:31
有了深度学习，只要学习的数据量足够大，计算机就可以自己提取动物的特征，并以此为基础进行图像的识别分类。所以说，深度学习的厉害之处就在于它可以自发地进行特征学习，这样人类就从繁杂的工作中解脱出来了。
2023-07-07 12:33
深度学习采取的是“自编码”，这是一种信息压缩器。
2023-07-07 12:32
在这个过程中，输入、输出是人类可以看到的，但是中间层是隐藏起来看不到的。正是在中间层（隐层）的位置，计算机完成了自动识别特征的工作。例如，一幅像素为1024的图像，输入层和输出层都是1024个像素点，但是隐层有300个特征点。这种1024到300的压缩，就是统计学中常用的“主成分分析法”。到了第三层，数据点又被压缩到100个，这样逐层压缩，但是越压缩，特征越抽象也越准确。生成的抽象度和准确度都高的特征，使深度学习在输出时就可以正确地输出...
2023-07-07 12:34
二、人工智能的集大成者：ChatGPT
2023-07-07 12:34
（一）从PGC到UGC再到AIGC
2023-07-07 12:35
（二）从深度学习到大模型
2023-07-07 12:35
模型是将现实问题进行抽象化，抽象成数学公式。
2023-07-07 12:36
2021年8月，美国华裔科学家、谷歌云的首席科学家李飞飞院士联合100多位学者发表一份题为On the Opportunities and Risk of Foundation Models的研究报告，这份报告有200多页，该报告详细描述了当前大规模预训练模型面临的机遇和挑战。在报告中，大模型被统一命名为Foundation Models，中文翻译为基础模型或者基石模型。该报告肯定了Foundation Models对智能体基本认知能力...
2023-07-07 12:36
（三）从“+人工智能”到“人工智能+”
2023-07-07 12:37
三、ChatGPT推动人工智能发展
2023-07-07 12:37
（一）ChatGPT是否代表一个新时代的开启每
2023-07-07 12:37
（二）ChatGPT后人工智能未来的发展方向人
第四章 ChatGPT的典型应用
2023-07-07 12:37
第四章 ChatGPT的典型应用C
一、文本生成中的ChatGPT
2023-07-07 12:38
一、文本生成中的ChatGPT
2023-07-07 12:38
（一）文本是怎样产生的？
2023-07-07 12:40
GPT-2的预训练数据规模约40GB，约有100亿个标记；GPT-3的预训练数据是由45TB的原始语料清洗而来，数据规模达570GB，约有4900亿个标记。GPT-2模型参数量为15亿，GPT-3参数量为1750亿。
2023-07-07 12:40
（二）ChatGPT生成文本的效果
2023-07-07 12:42
（三）ChatGPT将是文本生成的主力军吗？
2023-07-07 12:43
二、软件开发中的ChatGPT
2023-07-07 12:43
（一）ChatGPT还可以码代码！
2023-07-07 12:44
（二）ChatGPT用来开发软件的优势
2023-07-07 12:45
（三）ChatGPT和程序员是竞争还是合作？
2023-07-07 12:45
三、ChatGPT带来的其他可能
2023-07-07 12:45
（一）ChatGPT和图像处理
2023-07-07 12:47
（二）ChatGPT和语音合成
2023-07-07 12:48
（三）ChatGPT和自动驾驶
第五章 ChatGPT与Web3.0
2023-07-07 12:48
第五章 ChatGPT与Web3.0
一、从Web1.0、Web2.0到Web3.0
2023-07-07 12:49
Web3.0，又称“下一代互联网”，表示互联网发展进入新阶段。它是以区块链等技术为基础，以用户个人数据完全回归个人为前提的智能化、去中心化的全新互联网世界，相较于现在的互联网，会更为开放及实用。
2023-07-07 12:49
一、从Web1.0、Web2.0到Web3.0
2023-07-07 12:49
（一）“只读”的Web1.0
2023-07-07 12:50
Web1.0是个人计算机互联网，泛指20世纪90年代及21世纪初的web版本，它以门户网站为代表，降低了信息获取的门槛，提升了全球信息传输的效率。
2023-07-07 12:51
这个阶段，网络用户大部分都是内容消费者，内容创作者很少，主要是政府、企业等的门户网站，信息的传递以静态、单向为主，并且呈现中心化的特点，信息发布的主动权也在于网站的拥有者，用户除了“看”或“不看”的选择外，不具备任何自主权，基本上无法参与到互联网建设中。
2023-07-07 12:51
（二）“可读可写”的Web2.0
2023-07-07 12:52
在Web2.0中，用户不再局限于浏览，而是可以将创作自己的内容上传至网页上，
2023-07-07 12:52
智能终端（特别是智能手机）的普及和社交网络的兴起更是促成了Web2.0的急剧增长。
2023-07-07 12:52
Web2.0就是开启了一个新时代，全面激发了大众对文化创造的热情，更多的用户参与到了文化现象或文化事件的创造、分享和传播中，
2023-07-07 12:53
和Web1.0时代不同，用户可以在网站上创作并发布属于自己的内容（包括文字、图片、视频等），并与其他用户进行交流互动。
2023-07-07 12:53
Web2.0具有由中心化服务组成的特点，用户创作的内容、个人隐私数据等信息都集中在提供服务的特定公司，因为在获取各平台提供的服务之前，往往都会被“胁迫”同意个人数据采集，致使平台收集了大量的用户数据，这种数据集中于一处的方式存在着极大的网络安全隐患和数据垄断问题，个人信息一旦泄露，将会为用户带来极大的危害。
2023-07-07 12:53
除了信息泄露的风险外，更有一些平台利用采集的数据，用算法分析后进行“杀熟”，同样的商品，新用户看到的价格比老用户看到的价格低，优惠多。
2023-07-07 12:54
Web2.0时期是以用户生产和分享内容为主导的全新互动网络模式。虽然用户自己制作分享内容，但相关规则依然由互联网平台制定，即便用户给平台带来了数据和流量，由此转化成的经济效益却跟用户无关，其作为互联网价值的源头享受不到相应的价值收益。当
2023-07-07 12:54
（三）“可读可写可拥有”的Web3.0
2023-07-07 12:54
Web3.0致力于解决Web2.0存在的数据垄断、隐私保护缺失、算法作恶等问题，改变用户与互联网平台之间的不对等关系，让用户拥有更多自主权，在创造价值的同时又能公平地得到相应的利益，使互联网朝着更加开放但又普惠和安全的方向前进，成为更可信、更公平、更智能的新一代互联网。其核心思想就是去中心化，强调以用户为中心，让用户拥有更多自主权，即“可读可写可拥有的Web”，是一个运行在“区块链”技术之上的“去中心化”的互联网。
2023-07-07 13:39
它可以为用户打造一个独有的去中心化的通用数字身份体系，能在所有平台上使用，避免了创建账户设置密码的诸多问题。这个通用的数字身份体系是通过公私钥的签名与验签机制建立的，同时还通过分布式账本技术构建分布式公钥基础设施（Distributed Public Key Infrastructure, DPKI）和一种全新的可信分布式数字身份管理系统。用户数据经密码算法保护后在分布式账本上存储。身份信息与谁共享、作何种用途均由用户决定，只有经用户签...
2023-07-07 13:39
除了需要在不同的平台创建账号外，Web2.0时代，各个平台之间的信息也不能互通，数据完全独立。而Web3.0可以基于统一的通信协议，让不同网站内的信息可以直接进行交互及整合，通过这种方式，一方面，用户在所有网站上可直接使用自己的数据，而不是像在Web2.0时代，哪怕是图片、视频或文本这些通用形式的内容也都需要先从一个平台下载后保存至本地，而后登录其他平台，再进行发布上传。
2023-07-07 13:40
另一方面，由于Web3.0各平台之间是基于统一的分布式协议进行连接，用户如果想更换平台也是极其简单的，只要花费极小的成本甚至无成本地就可以从一个提供商转移到另一个提供商。这是一种自由的双向选择关系，用户与平台完全处于平等地位，而不像Web2.0时代，由于平台之间的信息无法互通，也许最初用户还拥有选择的权利，但一旦开始注册使用，就被平台牢牢绑定，如果想更换平台，虽然也可以，但是意味着你要抛弃在当前平台上的所有数据，尤其是那些消失在历史洪流...
2023-07-07 13:40
大家目前广泛使用的各种转账功能，看似是点对点发送价值，其实不然，在其背后，是有可信赖机构的账户系统为大家背书，进行价值的登记、流转、清算与结算。而Web3.0可以保证用户安全地进行金融交易，而且无须集中授权或协调者，这同样依赖于分布式账本技术。分布式账本技术为数字资产提供了独一无二的权益证明。利用哈希算法辅以时间戳生成的序列号是数字资产唯一性的保证，难以被复制。一人记录、多人监督复核的分布式共识算法杜绝了在没有可信中间人的情况下数字资产...
2023-07-07 13:40
Web1.0和Web2.0以互联网平台为核心，由互联网平台组织开展信息生产与收集，通过平台连接产生网络效应，降低生产者与消费者之间的搜寻成本，优化供需匹配，因此被称为平台经济。而Web3.0利用分布式账本技术，构建一个激励相容的开放式环境，我们称之为去中心化自治组织（DAO）。在这样的环境中，众多互不相识的个体自愿参与“无组织”的分布式协同作业，像传统企业一样投资、运营、管理项目，并共同拥有权益和资产。项目决策依靠民主治理，由参与者共同...
2023-07-07 18:48
二、ChatGPT加快Web3.0来临
2023-07-07 18:48
（一）助力区块链技术
2023-07-07 18:49
智能合约与区块链的结合，普遍被认为是区块链世界中一次里程碑式的升级，它允许在没有第三方的情况下进行可信交易，大大简化了业务交易流程，在不影响真实性和可信度的情况下减少了付款延迟、错误风险和传统合同的复杂性。它是区块链项目发展过程中不可或缺的一部分，第一个结合了区块链与智能合约技术的平台——以太坊（Ethereum）的诞生，更是被认为开启了“区块链2.0”时代。
2023-07-07 18:49
（二）重塑内容生产方式
2023-07-07 18:50
以数字内容为对象，针对以下四个问题，就可以看出互联网发展模式的更迭，即谁创造了它？它属于谁？又是谁在管理、支配它？它创造的价值如何分配？Web1.0时代是“平台创造、平台所有、平台控制、平台受益”，Web2.0时代是“用户创造、平台所有、平台控制、平台分配收益”，Web3.0时代则是“用户创造、用户所有、用户控制、协议分配收益”。
2023-07-07 18:50
（三）更好地了解Web3.0
2023-07-07 18:51
我们面临的问题不是网上的信息太少，而是信息太多了，如何有效地筛选、提炼、整合这些信息，一直都是个难题。ChatGPT可以解决这个问题。它通过快速整合知识库里相关的信息，总结提炼出最符合我们需求的答案并呈现出来。整个过程会非常流畅、自然、舒适，可以省去我们在大量信息间来回穿梭、整理的时间。
2023-07-07 18:52
（四）启迪Web3.0发展之路
2023-07-07 18:53
三、Web3.0时代ChatGPT将持续进化
2023-07-07 18:53
（一）解决ChatGPT的隐私和安全问题
2023-07-07 18:53
（二）促进ChatGPT的预训练或微调
2023-07-07 18:54
（三）打造未来的AIGC管理模式
2023-07-07 18:54
有关著作权的法律规定，作者只能是自然人、法人或非法人组织。那么AIGC创作的主体是人工智能技术本身，并不具备满足“作者”的属性特征。
2023-07-07 18:55
多方利益角色形成的去中心化自治组织模型，也能适用于AIGC的平台管理。将创作者、原创艺术品所有者、AIGC运营商、区块链验证者放入四个角色板块，其中创作者产生的收益可以分配给其他三位参与者，收益份额由常规投票权决定。在这一模式中，商业价值来自创作者，并分配给原始艺术品所有者、AIGC运营商和区块链，这些参与者都可以通过Web3.0集成到一个去中心化自治组织中，让AIGC的使用与沟通更加便捷、高效。
2023-07-07 18:55
（四）更多的可能性
第六章 ChatGPT与内容时代
2023-07-07 18:56
第六章 ChatGPT与内容时代ChatGPT
一、内容驱动的商业模式
2023-07-07 18:56
一、内容驱动的商业模式
2023-07-07 19:01
（一）内容产业是什么？
2023-07-07 19:02
（二）经营内容的商业模式有哪些？
2023-07-07 19:03
广告模式的优点在于它可以吸引大量的流量，并提供一种比其他商业模式更为简单的收入来源。然而，广告模式的缺点在于广告对用户的体验有时可能会造成干扰，特别是对于过于密集或冗长的广告。此外，由于广告主需要投入大量的广告费用，他们可能会对广告的效果和收益有严格的要求。
2023-07-07 19:03
赞助模式的缺点在于它可能会使内容受到赞助商的影响和控制，这可能会降低内容的质量和可信度。此外，赞助模式还需要内容提供者花费大量的时间和资源来协调与管理赞助商的利益和目标。
2023-07-07 19:04
订阅模式的优点在于它提供了可靠的收入来源，可以鼓励内容提供者制作更高质量的内容。此外，订阅模式还可以帮助内容提供者更好地了解其受众，以便更好地制定营销和发展策略。订阅模式的缺点在于它需要用户每月支付一定的费用，这可能会使一些用户望而却步。
2023-07-07 19:04
联合营销模式是将多个公司或个人的资源和品牌合并在一起，以共同推广某个内容或活动。
2023-07-07 19:04
联合营销模式的缺点在于，合作伙伴之间可能会存在分歧和冲突，导致项目失败。此外，联合营销模式还需要大量的资源和时间来协调与管理各方的利益及目标。
2023-07-07 19:04
会员模式要求用户支付订阅费或会员费来访问特定的内容。
2023-07-07 19:04
会员模式的优点在于它可以为内容提供者提供稳定的收入来源，并为会员提供更好的体验和服务。此外，会员模式还可以帮助内容提供者更好地了解其受众，以便更好地制定营销和发展策略。会员模式的缺点在于它可能会限制用户的访问和体验，并可能会使一些用户选择其他免费或更便宜的替代品。此外，会员模式还需要内容提供者花费大量的时间和资源来开发与维护独家内容及服务。
2023-07-07 19:04
（三）内容时代的产业特征
2023-07-07 19:04
内容时代的产业特征包括创新、碎片化、互动性和数据驱动几个方面。
2023-07-07 19:05
二、ChatGPT的内容产生方式
2023-07-07 19:05
（一）ChatGPT对内容产生方式的变革
2023-07-07 19:06
（二）ChatGPT对内容运营模式的影响
2023-07-07 19:06
（三）ChatGPT对受众内容体验的提升
2023-07-07 19:06
三、ChatGPT让内容走向无尽
2023-07-07 19:07
（一）ChatGPT的内容联想能力
2023-07-07 19:07
（二）ChatGPT的内容创造力
2023-07-07 19:08
（三）ChatGPT能取代人类文明吗？
2023-07-07 19:09
简而言之，世界上有很多问题，其中只有一小部分是数学问题；而在数学问题中，只有一小部分是有解的；在有解的问题中，只有部分是理想状态的计算机能解决的；而人工智能或者说ChatGPT可以解决的问题又是计算机可以解决问题的部分。ChatGPT能解决的问题都在其边界以内。人类和人工智能各有其优势，人类有无限的思维能力和创造力，人工智能有强大的算力和存储力，
2023-07-07 19:09
从技术角度来看，ChatGPT本质上只是一种基于机器学习的算法，它缺乏人类的情感和判断能力，也无法具备人类的智慧和创造力。
第七章 ChatGPT与搜索引擎
2023-07-07 19:09
第七章 ChatGPT与搜索引擎ChatGPT
一、搜索引擎及其技术架构
2023-07-08 14:51
一、搜索引擎及其技术架构
2023-07-08 14:52
（一）什么是搜索引擎
2023-07-08 15:06
1990年，由加拿大蒙特利尔麦吉尔大学的三名学生艾伦·埃姆蒂奇（Alan Emtage）、彼得·道奇（Peter Deutsch）、比尔·惠兰（Bill Wheelan）发明的Archie（Archie FAQ）被视为搜索引擎的鼻祖。
2023-07-08 15:06
最初的Archie实际上是第一个自动索引互联网上匿名FTP（文件传输协议）网站文件的程序，它和我们今天所使用的真正意义上的搜索引擎还有着很大差距。尽管如此，Archie的诞生仍然有着跨时代的意义。Archie有一个可搜索的FTP文件名列表，用户必须输入精确的文件名搜索，然后Archie会告诉用户哪一个FTP地址可以下载该文件。
2023-07-08 15:06
第一代搜索引擎——分类目录时代。
2023-07-08 15:06
第二代搜索引擎——文本检索时代。
2023-07-08 15:06
第三代搜索引擎——整合分析时代。
2023-07-08 15:06
第四代搜索引擎——用户中心时代。
2023-07-08 15:07
（二）搜索引擎的类型
2023-07-08 15:09
（三）搜索引擎的技术架构
2023-07-08 15:10
二、ChatGPT与搜索引擎的区别
2023-07-08 15:10
（一）ChatGPT与搜索引擎在功能上的区别我
2023-07-08 15:10
我们现在正处在一个信息过载的时代，全世界每年产生1EB（艾字节）到2EB信息，相当于地球上每个人每年大概产生250MB（兆字节）信息。其中，纸质信息仅占所有信息的0.03%。静态网页有上百亿字节，动态及隐藏网页至少是静态网页的500倍。汤姆·兰道尔（Tom Landauer）认为人的大脑只能存储约200MB信息，一生只能接触约6GB信息。
2023-07-08 15:11
（二）ChatGPT和搜索引擎在技术实现上的不同之处
2023-07-08 15:12
三、ChatGPT改变搜索引擎？
2023-07-08 15:12
（一）ChatGPT会取代搜索引擎吗？
2023-07-08 15:13
目前英文版本的ChatGPT数据截至2021年，而中文版本的ChatGPT数据截至2020年，
2023-07-08 15:13
ChatGPT目前在GPT大模型上加入标注数据训练的模式让实时数据的引入非常困难，如果要重新预训练模型，估计每次预训练需要用到1000块以上的英伟达A100显卡工作半个月至一个月的时间，成本在百万美元以上。而如果采用微调的方式专门训练新知识，会导致新知识在模型内的权重过高，频繁的微调也会导致模型“遗忘”旧的知识。
2023-07-08 15:14
根据模型的现有数据，我们假设每次生成的回答长度平均为50个词，用于推理的情况下，我们估算ChatGPT每一次生成答案的成本约为1.3美分，约为谷歌搜索引擎每次搜索成本的3倍。如果每天面对数以亿计用户的搜索请求，如此高昂的成本是研发公司所不能承受的，在中短期内ChatGPT完全取代传统搜索引擎在商业模式上无法做到。
2023-07-08 15:14
（二）搜索引擎变革就在眼前
2023-07-08 15:16
考虑到ChatGPT在不同分类问题中的表现情况，可以限制ChatGPT搜索，仅在知识类搜索场景下启用，这样可以有效控制成本。面对时效类问题时，模型自动判断转向传统搜索引擎生成答案，并通过传统搜索引擎的数据返回生成ChatGPT版本的汇总新答案。
2023-07-08 16:02
李彦宏表示：“现在之所以说搜索本质上是一个人工智能的问题，原因就在于，当人们用文字、问题提出请求或者表达兴趣的时候，计算机会推测人类或用户的意图，从而提供相关答案。而这就是人工智能的本质，即让计算机了解人类、服务人类。”
第八章 ChatGPT与元宇宙
2023-07-08 16:03
第八章 ChatGPT与元宇宙
一、什么是元宇宙？
2023-07-08 16:03
一、什么是元宇宙？
2023-07-08 16:04
元宇宙，由“Metaverse”一词直译而来，最早出现在美国著名作家尼尔·史蒂芬森（Neal Stephenson）的科幻小说《雪崩》之中，由“meta”（超越，高于）+“verse”（Universe，宇宙）组成，有“超越宇宙”，即“宇宙的宇宙”之义。
2023-07-08 16:04
（一）元宇宙的概念认知
2023-07-08 16:05
（二）元宇宙的发展阶段
2023-07-08 16:08
（三）元宇宙的未来之路
2023-07-08 16:08
二、元宇宙中的ChatGPT
2023-07-08 16:08
（一）通往元宇宙的技术途径
2023-07-08 16:09
（二）元宇宙与ChatGPT的关系
2023-07-08 16:09
（三）ChatGPT在元宇宙中的应用
2023-07-08 16:10
三、ChatGPT会带来元宇宙新契机吗？
2023-07-08 16:10
美国元宇宙代表Meta公司进行了大规模裁员和预算削减，微软也砍掉了成立仅四个月的工业元宇宙团队；在国内，元宇宙也慢慢回落遇冷，腾讯裁撤XR相关人员和项目，字节跳动关闭了元宇宙社交App——派对岛，不少之前的元宇宙社群更名为ChatGPT社群。
2023-07-08 16:10
（一）ChatGPT加速了元宇宙的实现
2023-07-08 16:11
（二）ChatGPT让元宇宙昙花一现？
2023-07-08 16:11
（三）元宇宙与ChatGPT联动发展的意见和建议
第九章 ChatGPT与信息安全
2023-07-08 16:12
第九章 ChatGPT与信息安全ChatGPT
一、互联网时代的信息安全
2023-07-08 16:12
一、互联网时代的信息安全
2023-07-08 16:13
（一）照片or“照骗”
2023-07-08 16:14
（二）大数据与隐私
2023-07-08 16:14
二、ChatGPT带来的信息安全隐患事
2023-07-08 16:15
（一）你手机的另一端是谁？
2023-07-08 16:15
1950年，图灵发表《计算机器与智能》一文，他在文中提出：假设让计算机来冒充人，与人对话，如果不足70%的人判对，即判断对面是机器，而有超过30%的裁判判断对面和自己对话的是人而非计算机，那就算作成功了，相当于100分的试卷，机器只需要考30分就通过图灵测试了。
2023-07-08 16:17
（二）我会被带到沟里吗？
2023-07-08 16:18
（三）价值渗透怎么办？
2023-07-08 16:19
三、能否走出智能与安全困局？
2023-07-08 16:19
（一）智能领域漏洞无处不在
2023-07-08 16:20
（二）智能与安全的平衡点在哪里？
第十章 ChatGPT加速产业升级
2023-07-08 16:20
第十章 ChatGPT加速产业升级
一、ChatGPT催生全新产业结构
2023-07-08 16:20
一、ChatGPT催生全新产业结构
2023-07-08 16:20
（一）ChatGPT在医疗服务领域的应用
2023-07-08 16:21
（二）ChatGPT加速政府数字化转型
2023-07-08 16:21
（三）ChatGPT潜在军事应用分析
2023-07-08 16:21
二、模型即服务形成新的业态
2023-07-08 16:22
模型即服务，英文简称是MaaS（Model as a Service）。大模型，又称为预训练模型、基础模型等，是“大算力+强算法”结合的产物。
2023-07-08 16:22
（一）大模型科技公司抢占人工智能大模型业态新高地
2023-07-08 16:23
2017年，谷歌的阿希什·瓦斯瓦尼（Ashish Vaswani）等提出Transformer架构，奠定了当前大模型领域主流的算法架构基础；Transformer结构的提出，使深度学习模型参数达到了上亿的规模。2018年，谷歌提出了大规模预训练语言模型Bert，该模型是基于Transformer的双向深层预训练模型，其参数首次超过3亿规模；同年，OpenAI提出了生成式预训练Transformer模型——GPT，大大地推动了自然语言处理...
2023-07-08 16:23
此后，微软和英伟达在2020年10月联手发布了拥有5300亿参数的自然语言生成模型MT-NLG。2021年1月，谷歌推出的Switch Transformer模型以高达1.6万亿的参数量成为史上首个万亿级语言模型；同年12月，谷歌还提出了1.2万亿参数的通用稀疏语言模型GLaM，其在7项小样本学习领域的性能超过GPT-3。可以看到，大型语言模型的参数数量保持着指数增长势头。这样高速的发展并没有结束，2022年，又有一些常规业态大模型涌现...
2023-07-08 16:24
（二）大模型驱动的AIGC产业应用创新
2023-07-08 16:24
三、ChatGPT还会带来什么？
2023-07-08 16:24
（一）ChatGPT的未来应用场景充满无限可能
2023-07-08 16:24
ChatGPT被公认为是继互联网、智能手机之后，带给人类的第三次革命性产品。互联网开辟了“空间革命”，使人类可以实时与全世界链接，不必奔赴现场，可以通过互联网进行沟通、教学、视频会议，使政治、社会和商业等领域发生连锁变化。智能手机的出现带来了“时间革命”，通过可拓展安装的各种App应用软件，可以实现最快交易、最速送达，为人类的生活、工作和消费带来巨大变化。ChatGPT的横空出世，有望形成“思维革命”，替代人类进行创作、创意、解答、咨询...
2023-07-08 16:25
（二）ChatGPT对产业的影响
结语 ChatGPT带来的机遇和挑战
2023-07-08 16:26
结语 ChatGPT带来的机遇和挑战
2023-07-08 16:26
ChatGPT海外爆红、服务器被挤垮，百万网友使用，2022年也因此被称为“AIGC元年”，生成式人工智能技术发展迅速。
2023-07-08 16:26
自2022年11月发布，仅仅两个月后，ChatGPT的活跃用户就已经达到1亿，打破了过往纪录，这个纪录TikTok用了9个月，Instagram则用了两年半。
当当云阅读笔记 来自当当云阅读 for Android
--dangdangbookid1901328019 for Android
